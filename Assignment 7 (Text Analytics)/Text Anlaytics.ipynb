{"cells":[{"cell_type":"markdown","id":"4937436c","metadata":{"id":"4937436c"},"source":["# Natural Langauage Processing (NLP)"]},{"cell_type":"markdown","id":"e1f77f3d","metadata":{"id":"e1f77f3d"},"source":["### NLTK installation\n","\n","**pip3 install nltk**\n","\n","**#import nltk**\n","\n","**#nltk.download()**"]},{"cell_type":"markdown","id":"88694ff2","metadata":{"id":"88694ff2"},"source":["## Tokenization of words:\n","\n","We use the method word_tokenize() to split a sentence into words.         \n","word tokenization becomes a crucial part of the text (string) to numeric data conversion"]},{"cell_type":"code","execution_count":1,"id":"a6f1cebd","metadata":{"id":"a6f1cebd","executionInfo":{"status":"ok","timestamp":1681897147029,"user_tz":-330,"elapsed":2346,"user":{"displayName":"Aishwarya Shinde","userId":"13687257978232432507"}}},"outputs":[],"source":["import nltk\n","#nltk.download()"]},{"cell_type":"code","source":["import nltk\n","nltk.download('punkt')"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"of0iiwZItlrw","executionInfo":{"status":"ok","timestamp":1681897163684,"user_tz":-330,"elapsed":1220,"user":{"displayName":"Aishwarya Shinde","userId":"13687257978232432507"}},"outputId":"35df0a5c-8d58-4437-ac12-a9c78a67e7ff"},"id":"of0iiwZItlrw","execution_count":3,"outputs":[{"output_type":"stream","name":"stderr","text":["[nltk_data] Downloading package punkt to /root/nltk_data...\n","[nltk_data]   Unzipping tokenizers/punkt.zip.\n"]},{"output_type":"execute_result","data":{"text/plain":["True"]},"metadata":{},"execution_count":3}]},{"cell_type":"markdown","id":"cc395453","metadata":{"id":"cc395453"},"source":["## Word Tokenizer"]},{"cell_type":"code","execution_count":5,"id":"30e7386f","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"30e7386f","executionInfo":{"status":"ok","timestamp":1681897174400,"user_tz":-330,"elapsed":9,"user":{"displayName":"Aishwarya Shinde","userId":"13687257978232432507"}},"outputId":"a978895e-e5bc-4637-ea53-7d370c346dd3"},"outputs":[{"output_type":"stream","name":"stdout","text":["['Welcome', 'to', 'the', 'Python', 'Programming', 'at', 'Indeed', 'Insprining', 'Infotech']\n"]}],"source":["import nltk\n","from nltk.tokenize import word_tokenize\n","text = \"Welcome to the Python Programming at Indeed Insprining Infotech\"\n","print(word_tokenize(text))"]},{"cell_type":"markdown","id":"c9071bd0","metadata":{"id":"c9071bd0"},"source":["## Sentence Tokenizer"]},{"cell_type":"code","execution_count":6,"id":"c00a2c90","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"c00a2c90","executionInfo":{"status":"ok","timestamp":1681897183650,"user_tz":-330,"elapsed":536,"user":{"displayName":"Aishwarya Shinde","userId":"13687257978232432507"}},"outputId":"5aaa3611-31ea-4984-efca-6b8bf3380cb9"},"outputs":[{"output_type":"stream","name":"stdout","text":["['Hello Everyone.', 'Welcome to the Python Programming']\n"]}],"source":["from nltk.tokenize import sent_tokenize\n","text = \"Hello Everyone. Welcome to the Python Programming\"\n","print(sent_tokenize(text))"]},{"cell_type":"markdown","id":"3512ea5b","metadata":{"id":"3512ea5b"},"source":["## Stemming\n","When we have many variations of the same word for example...the word is dance and the variations are \"dancing\", \"dances\",\"danced\".             \n","Stemming algorithm works by cutting the suffix from the word."]},{"cell_type":"code","execution_count":7,"id":"484f9a1a","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"484f9a1a","executionInfo":{"status":"ok","timestamp":1681897209249,"user_tz":-330,"elapsed":500,"user":{"displayName":"Aishwarya Shinde","userId":"13687257978232432507"}},"outputId":"4e8fa6d4-d2f5-4aee-8bde-3744e2cf9734"},"outputs":[{"output_type":"stream","name":"stdout","text":["clean\n","clean\n","clean\n","clean\n"]}],"source":["from nltk.stem import PorterStemmer\n","# words = ['Wait','Waiting','Waited','Waits']\n","words = ['clean','cleaning','cleans','cleaned']\n","ps = PorterStemmer()\n","for w in words:\n","    words=ps.stem(w)\n","    print(words)"]},{"cell_type":"markdown","id":"a6a26280","metadata":{"id":"a6a26280"},"source":["## lemmatization\n","\n","### Why is Lemmatization better than Stemming?\n","**Stemming algorithm woks by cutting the suffix from the word and Lemmatization is a more powerful operation because it perform morphological analysis of the words.**"]},{"cell_type":"markdown","id":"f6c220c5","metadata":{"id":"f6c220c5"},"source":["## Stemming Code:"]},{"cell_type":"code","execution_count":8,"id":"757a2a87","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"757a2a87","executionInfo":{"status":"ok","timestamp":1681897226308,"user_tz":-330,"elapsed":479,"user":{"displayName":"Aishwarya Shinde","userId":"13687257978232432507"}},"outputId":"06f0d42b-037c-4e3a-a2e7-cc53adec6422"},"outputs":[{"output_type":"stream","name":"stdout","text":["Stemming for  studies is studi\n","Stemming for  studying is studi\n","Stemming for  floors is floor\n","Stemming for  cry is cri\n"]}],"source":["import nltk\n","from nltk.stem.porter import PorterStemmer\n","porter_stemmer = PorterStemmer()\n","text  = \"studies studying floors cry\"\n","tokenization = nltk.word_tokenize(text)\n","for w in tokenization:\n","    print('Stemming for ', w,'is',porter_stemmer.stem(w))"]},{"cell_type":"markdown","id":"af1028ed","metadata":{"id":"af1028ed"},"source":["## lemmatization Code:"]},{"cell_type":"code","source":["import nltk\n","nltk.download('wordnet')\n","nltk.download('omw-1.4')"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"Z1b39DQqtz27","executionInfo":{"status":"ok","timestamp":1681897238913,"user_tz":-330,"elapsed":498,"user":{"displayName":"Aishwarya Shinde","userId":"13687257978232432507"}},"outputId":"83df0852-72f7-49ff-e3af-d6903d8fd00d"},"id":"Z1b39DQqtz27","execution_count":9,"outputs":[{"output_type":"stream","name":"stderr","text":["[nltk_data] Downloading package wordnet to /root/nltk_data...\n","[nltk_data] Downloading package omw-1.4 to /root/nltk_data...\n"]},{"output_type":"execute_result","data":{"text/plain":["True"]},"metadata":{},"execution_count":9}]},{"cell_type":"code","execution_count":10,"id":"d2525888","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"d2525888","executionInfo":{"status":"ok","timestamp":1681897243384,"user_tz":-330,"elapsed":1629,"user":{"displayName":"Aishwarya Shinde","userId":"13687257978232432507"}},"outputId":"8df98fb3-800f-4420-c44d-8de1972806df"},"outputs":[{"output_type":"stream","name":"stdout","text":["Lemma for  studies is study\n","Lemma for  study is study\n","Lemma for  floors is floor\n","Lemma for  cry is cry\n"]}],"source":["import nltk\n","from nltk.stem import WordNetLemmatizer\n","Wordnet_lemmatizer = WordNetLemmatizer()\n","text  = \"studies study floors cry\"\n","tokenization = nltk.word_tokenize(text)\n","for w in tokenization:\n","    print('Lemma for ', w,'is',Wordnet_lemmatizer.lemmatize(w))"]},{"cell_type":"markdown","id":"106eac46","metadata":{"id":"106eac46"},"source":["## NLTK stop words\n","\n","**Text may contain stop words like 'the','is','are','a'. Stop words can be filterd from the text to be processed.**"]},{"cell_type":"code","source":["nltk.download('stopwords')"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"IJR_5QtwuDsa","executionInfo":{"status":"ok","timestamp":1681897249872,"user_tz":-330,"elapsed":527,"user":{"displayName":"Aishwarya Shinde","userId":"13687257978232432507"}},"outputId":"cd928421-5526-495f-a130-aa28c5322b1a"},"id":"IJR_5QtwuDsa","execution_count":11,"outputs":[{"output_type":"stream","name":"stderr","text":["[nltk_data] Downloading package stopwords to /root/nltk_data...\n","[nltk_data]   Unzipping corpora/stopwords.zip.\n"]},{"output_type":"execute_result","data":{"text/plain":["True"]},"metadata":{},"execution_count":11}]},{"cell_type":"code","execution_count":12,"id":"ae25d08f","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"ae25d08f","executionInfo":{"status":"ok","timestamp":1681897255057,"user_tz":-330,"elapsed":7,"user":{"displayName":"Aishwarya Shinde","userId":"13687257978232432507"}},"outputId":"63ccf7ee-62de-405f-ca3b-f88c4c790e69"},"outputs":[{"output_type":"stream","name":"stdout","text":["['AI', 'introduced', 'year', '1956', 'gained', 'popularity', 'recently', '.']\n"]}],"source":["from nltk.tokenize import sent_tokenize, word_tokenize\n","from nltk.corpus import stopwords\n","\n","data = 'AI was introduced in the year 1956 but it gained popularity recently.'\n","stopwords = set(stopwords.words('english'))\n","words = word_tokenize(data)\n","wordsFiltered = []\n","\n","for w in words:\n","    if w not in stopwords:\n","        wordsFiltered.append(w)\n","        \n","print(wordsFiltered)"]},{"cell_type":"code","execution_count":13,"id":"b3df1cb3","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"b3df1cb3","executionInfo":{"status":"ok","timestamp":1681897271884,"user_tz":-330,"elapsed":8,"user":{"displayName":"Aishwarya Shinde","userId":"13687257978232432507"}},"outputId":"caf94a46-6329-4752-a3a6-6006722ebafe"},"outputs":[{"output_type":"stream","name":"stdout","text":["179\n","{'were', 'are', \"isn't\", 'this', 'herself', 'until', 'under', 'that', 'here', 'over', 'by', 'while', 'was', 'not', 'most', 'had', 'be', 'can', 'because', \"mustn't\", 'just', 'hadn', 'yours', 'haven', 'is', 'do', 'a', 'about', 'should', 'above', 'm', 'if', 'how', 'hers', 'yourselves', 'her', 'themselves', 'as', 'of', 'at', 'ourselves', 'has', 'won', 've', \"wasn't\", 'all', 'it', 'itself', 't', \"you'd\", 'which', 'what', \"doesn't\", 'there', 'y', \"you've\", \"needn't\", 'their', 'been', 'does', 'myself', 'out', 'when', \"hasn't\", \"wouldn't\", 'ain', 'each', 'then', 'ours', 'we', 'its', 'up', 'such', 'ma', \"aren't\", 'his', \"she's\", \"you'll\", \"shouldn't\", 'whom', 'on', 'before', 'some', 'they', 'down', 'an', 'again', 'him', 'he', 'am', 'wasn', 'into', 'nor', 'you', 'after', 'our', 'other', 'them', 'no', 'so', 'don', \"that'll\", 'from', 'between', 'in', 'during', 'have', 'mustn', 'both', 'to', 'isn', 'yourself', 'mightn', 'own', 'further', 'through', 'didn', 'but', \"weren't\", 'd', 'will', \"mightn't\", 'or', 'shouldn', 'your', 'did', 'me', \"you're\", 'the', 'aren', 'these', \"it's\", \"couldn't\", 'hasn', \"didn't\", 'my', 'few', 'very', 'why', 'below', 'than', 'doesn', 'she', 'doing', \"should've\", 'same', 'more', 'i', 'couldn', 'and', 'those', 'being', 're', \"haven't\", \"don't\", 'shan', 'only', 'for', 'once', \"shan't\", 'any', 'weren', 'theirs', 'now', \"won't\", 'who', 'with', 'needn', 'wouldn', 'o', 'against', 'himself', 's', 'off', 'too', 'll', 'having', \"hadn't\", 'where'}\n"]}],"source":["print(len(stopwords))\n","print(stopwords)"]},{"cell_type":"markdown","id":"0798460f","metadata":{"id":"0798460f"},"source":["# Text Analytics\n","\n","1. Extract Sample document and apply following document preprocessing methods: Tokenization, POS Tagging, stop words removal, Stemming and Lemmatization.\n","2. Create representation of document by calculating Term Frequency and Inverse Document Frequency."]},{"cell_type":"code","execution_count":14,"id":"2c86e8eb","metadata":{"id":"2c86e8eb","executionInfo":{"status":"ok","timestamp":1681897280071,"user_tz":-330,"elapsed":1153,"user":{"displayName":"Aishwarya Shinde","userId":"13687257978232432507"}}},"outputs":[],"source":["import nltk\n","from nltk.tokenize import word_tokenize\n","from nltk.corpus import stopwords\n","from nltk.stem import PorterStemmer, WordNetLemmatizer\n","from nltk import pos_tag"]},{"cell_type":"code","execution_count":15,"id":"984ab773","metadata":{"id":"984ab773","executionInfo":{"status":"ok","timestamp":1681897329709,"user_tz":-330,"elapsed":493,"user":{"displayName":"Aishwarya Shinde","userId":"13687257978232432507"}}},"outputs":[],"source":["# Example document\n","document = \"This is an example document that we will use to demonstrate document preprocessing.\""]},{"cell_type":"code","execution_count":16,"id":"e2d87a82","metadata":{"id":"e2d87a82","executionInfo":{"status":"ok","timestamp":1681897332433,"user_tz":-330,"elapsed":5,"user":{"displayName":"Aishwarya Shinde","userId":"13687257978232432507"}}},"outputs":[],"source":["# Tokenization\n","tokens = word_tokenize(document)"]},{"cell_type":"code","execution_count":17,"id":"ca8b7015","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"ca8b7015","executionInfo":{"status":"ok","timestamp":1681897334711,"user_tz":-330,"elapsed":8,"user":{"displayName":"Aishwarya Shinde","userId":"13687257978232432507"}},"outputId":"d559b940-90ff-42e5-861f-60e3554dbefa"},"outputs":[{"output_type":"execute_result","data":{"text/plain":["['This',\n"," 'is',\n"," 'an',\n"," 'example',\n"," 'document',\n"," 'that',\n"," 'we',\n"," 'will',\n"," 'use',\n"," 'to',\n"," 'demonstrate',\n"," 'document',\n"," 'preprocessing',\n"," '.']"]},"metadata":{},"execution_count":17}],"source":["tokens"]},{"cell_type":"code","execution_count":18,"id":"0e184c68","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"0e184c68","executionInfo":{"status":"ok","timestamp":1681897337172,"user_tz":-330,"elapsed":6,"user":{"displayName":"Aishwarya Shinde","userId":"13687257978232432507"}},"outputId":"5d94ad6d-80fe-452e-c25a-167ae21e21fc"},"outputs":[{"output_type":"stream","name":"stderr","text":["[nltk_data] Downloading package averaged_perceptron_tagger to\n","[nltk_data]     /root/nltk_data...\n","[nltk_data]   Unzipping taggers/averaged_perceptron_tagger.zip.\n"]},{"output_type":"execute_result","data":{"text/plain":["True"]},"metadata":{},"execution_count":18}],"source":["import nltk\n","nltk.download('averaged_perceptron_tagger')"]},{"cell_type":"code","execution_count":19,"id":"cc62bc9b","metadata":{"id":"cc62bc9b","executionInfo":{"status":"ok","timestamp":1681897340532,"user_tz":-330,"elapsed":4,"user":{"displayName":"Aishwarya Shinde","userId":"13687257978232432507"}}},"outputs":[],"source":["# POS tagging\n","#These tags can indicate whether a word is a noun, verb, adjective, adverb, preposition, conjunction, or other grammatical category.\n","pos_tags = pos_tag(tokens)"]},{"cell_type":"code","source":["pos_tags\n","# DT for determiner , NN for noun , VBD for past tense word , IN for preprosition\n","#VBZ: verb, 3rd person singular present tense (e.g. \"runs\")\n","# VB: verb, base form (e.g. \"run\")\n","# PRP: personal pronoun (e.g. \"he\", \"she\", \"it\", \"they\")\n","# MD: modal verb (e.g. \"can\", \"should\", \"will\")\n","# TO: to (e.g. \"to run\", \"to go\")\n","# Here are some additional tags that you may find useful:\n","\n","# NN: noun, singular or mass (e.g. \"cat\", \"dog\", \"water\")\n","# NNS: noun, plural (e.g. \"cats\", \"dogs\")\n","# JJ: adjective (e.g. \"happy\", \"blue\")\n","# RB: adverb (e.g. \"quickly\", \"very\")\n","# IN: preposition or subordinating conjunction (e.g. \"in\", \"on\", \"because\")"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"_O5mmUuouPid","executionInfo":{"status":"ok","timestamp":1681897343119,"user_tz":-330,"elapsed":6,"user":{"displayName":"Aishwarya Shinde","userId":"13687257978232432507"}},"outputId":"5d37fac2-30e8-4d44-85e5-ff6cccba7959"},"id":"_O5mmUuouPid","execution_count":20,"outputs":[{"output_type":"execute_result","data":{"text/plain":["[('This', 'DT'),\n"," ('is', 'VBZ'),\n"," ('an', 'DT'),\n"," ('example', 'NN'),\n"," ('document', 'NN'),\n"," ('that', 'IN'),\n"," ('we', 'PRP'),\n"," ('will', 'MD'),\n"," ('use', 'VB'),\n"," ('to', 'TO'),\n"," ('demonstrate', 'VB'),\n"," ('document', 'NN'),\n"," ('preprocessing', 'NN'),\n"," ('.', '.')]"]},"metadata":{},"execution_count":20}]},{"cell_type":"code","execution_count":21,"id":"86c4662e","metadata":{"id":"86c4662e","executionInfo":{"status":"ok","timestamp":1681897344909,"user_tz":-330,"elapsed":6,"user":{"displayName":"Aishwarya Shinde","userId":"13687257978232432507"}}},"outputs":[],"source":["# Stopwords removal\n","stop_words = set(stopwords.words('english'))\n","filtered_tokens = [word for word in tokens if not word.lower() in stop_words]"]},{"cell_type":"code","execution_count":22,"id":"8aabf51c","metadata":{"id":"8aabf51c","executionInfo":{"status":"ok","timestamp":1681897348549,"user_tz":-330,"elapsed":6,"user":{"displayName":"Aishwarya Shinde","userId":"13687257978232432507"}}},"outputs":[],"source":["# Stemming\n","ps = PorterStemmer()\n","stemmed_tokens = [ps.stem(word) for word in filtered_tokens]"]},{"cell_type":"code","execution_count":23,"id":"4a6f5953","metadata":{"id":"4a6f5953","executionInfo":{"status":"ok","timestamp":1681897351206,"user_tz":-330,"elapsed":4,"user":{"displayName":"Aishwarya Shinde","userId":"13687257978232432507"}}},"outputs":[],"source":["# Lemmatization\n","wnl = WordNetLemmatizer()\n","lemmatized_tokens = [wnl.lemmatize(word) for word in filtered_tokens]"]},{"cell_type":"code","execution_count":24,"id":"5cd5d204","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"5cd5d204","executionInfo":{"status":"ok","timestamp":1681897356915,"user_tz":-330,"elapsed":902,"user":{"displayName":"Aishwarya Shinde","userId":"13687257978232432507"}},"outputId":"8f30b6f6-fc48-4569-a6e1-c414aabb1371"},"outputs":[{"output_type":"stream","name":"stdout","text":["Tokens:  ['This', 'is', 'an', 'example', 'document', 'that', 'we', 'will', 'use', 'to', 'demonstrate', 'document', 'preprocessing', '.']\n","Filtered tokens:  ['example', 'document', 'use', 'demonstrate', 'document', 'preprocessing', '.']\n","Stemmed tokens:  ['exampl', 'document', 'use', 'demonstr', 'document', 'preprocess', '.']\n","Lemmatized tokens:  ['example', 'document', 'use', 'demonstrate', 'document', 'preprocessing', '.']\n"]}],"source":["# Print the results\n","print(\"Tokens: \", tokens)\n","# print(\"POS tags: \", pos_tags)\n","print(\"Filtered tokens: \", filtered_tokens)\n","print(\"Stemmed tokens: \", stemmed_tokens)\n","print(\"Lemmatized tokens: \", lemmatized_tokens)\n","#NLTK is capable of performing all the document preprocessing methods that you have mentioned."]},{"cell_type":"markdown","id":"6ca1a978","metadata":{"id":"6ca1a978"},"source":["## Text Analytics\n","Create representation of document by calculating Term Frequency and Inverse Document Frequency."]},{"cell_type":"code","execution_count":null,"id":"375692a0","metadata":{"id":"375692a0"},"outputs":[],"source":["import math\n","from collections import Counter"]},{"cell_type":"code","source":["# Sample corpus of documents\n","corpus = [\n","    'The quick brown fox jumps over the lazy dog',\n","    'The brown fox is quick',\n","    'The lazy dog is sleeping'\n","]"],"metadata":{"id":"uX0WL3Ma4HTZ"},"id":"uX0WL3Ma4HTZ","execution_count":null,"outputs":[]},{"cell_type":"code","source":["# Tokenize the documents\n","tokenized_docs = [doc.lower().split() for doc in corpus]"],"metadata":{"id":"9iNWSB034S6I"},"id":"9iNWSB034S6I","execution_count":null,"outputs":[]},{"cell_type":"code","source":["# Count the term frequency for each document\n","tf_docs = [Counter(tokens) for tokens in tokenized_docs]"],"metadata":{"id":"TO8oDyF_4VR4"},"id":"TO8oDyF_4VR4","execution_count":null,"outputs":[]},{"cell_type":"code","source":["# Calculate the inverse document frequency for each term\n","n_docs = len(corpus)\n","idf = {}\n","for tokens in tokenized_docs:\n","    for token in set(tokens):\n","        idf[token] = idf.get(token, 0) + 1\n","for token in idf:\n","    idf[token] = math.log(n_docs / idf[token])"],"metadata":{"id":"mwbYVp9x4Y-B"},"id":"mwbYVp9x4Y-B","execution_count":null,"outputs":[]},{"cell_type":"code","source":["# Calculate the TF-IDF weights for each document\n","tfidf_docs = []\n","for tf_doc in tf_docs:\n","    tfidf_doc = {}\n","    for token, freq in tf_doc.items():\n","        tfidf_doc[token] = freq * idf[token]\n","    tfidf_docs.append(tfidf_doc)\n"],"metadata":{"id":"QF1Si3vv4bCE"},"id":"QF1Si3vv4bCE","execution_count":null,"outputs":[]},{"cell_type":"code","source":["# Print the resulting TF-IDF representation for each document\n","for i, tfidf_doc in enumerate(tfidf_docs):\n","    print(f\"Document {i+1}: {tfidf_doc}\")"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"cWDMLXpP4cZu","executionInfo":{"status":"ok","timestamp":1678705613598,"user_tz":-330,"elapsed":396,"user":{"displayName":"Aishwarya Shinde","userId":"13687257978232432507"}},"outputId":"fd841335-8e6c-4cbc-be1c-ab5228a915e0"},"id":"cWDMLXpP4cZu","execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["Document 1: {'the': 0.0, 'quick': 0.4054651081081644, 'brown': 0.4054651081081644, 'fox': 0.4054651081081644, 'jumps': 1.0986122886681098, 'over': 1.0986122886681098, 'lazy': 0.4054651081081644, 'dog': 0.4054651081081644}\n","Document 2: {'the': 0.0, 'brown': 0.4054651081081644, 'fox': 0.4054651081081644, 'is': 0.4054651081081644, 'quick': 0.4054651081081644}\n","Document 3: {'the': 0.0, 'lazy': 0.4054651081081644, 'dog': 0.4054651081081644, 'is': 0.4054651081081644, 'sleeping': 1.0986122886681098}\n"]}]},{"cell_type":"markdown","source":["This code uses the Counter class from the collections module to count the term frequency for each document. It then calculates the inverse document frequency by iterating over the tokenized documents and keeping track of the number of documents that each term appears in. Finally, it multiplies the term frequency of each term in each document by its corresponding inverse document frequency to get the TF-IDF weight for each term in each document. The resulting TF-IDF representation for each document is printed to the console."],"metadata":{"id":"FOSjIVpw4u9r"},"id":"FOSjIVpw4u9r"},{"cell_type":"code","source":[],"metadata":{"id":"No6doLX64dTF"},"id":"No6doLX64dTF","execution_count":null,"outputs":[]}],"metadata":{"kernelspec":{"display_name":"Python 3 (ipykernel)","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.9.7"},"colab":{"provenance":[]}},"nbformat":4,"nbformat_minor":5}